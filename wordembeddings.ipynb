{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading averaged_perceptron_tagger: <urlopen error\n",
      "[nltk_data]     [Errno 8] nodename nor servname provided, or not\n",
      "[nltk_data]     known>\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "# from sklearn.metrics import roc\n",
    "# from sklearn.model_selection import traintestsplit\n",
    "# from sklearn.preprocessing import \n",
    "import json\n",
    "import warnings\n",
    "import library as lib\n",
    "# from nltk.punkt impor\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "import string\n",
    "from nltk.stem.wordnet import WordNetLemmatizer, wordnet\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.collocations import *\n",
    "from nltk import FreqDist\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk import word_tokenize\n",
    "import re\n",
    "import os\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "warnings.filterwarnings(action='once')\n",
    "%matplotlib inline\n",
    "pd.set_option('display.max_columns', 999)\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/flatironschool/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Input, Dense, LSTM, Embedding\n",
    "from keras.layers import Dropout, Activation, Bidirectional, GlobalMaxPool1D\n",
    "from keras.models import Model\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "from keras.preprocessing import text, sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('preprocessed_data2.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "      <th>answers</th>\n",
       "      <th>plausible_answers</th>\n",
       "      <th>is_impossible</th>\n",
       "      <th>context_lemma_pos</th>\n",
       "      <th>question_lemma_pos</th>\n",
       "      <th>answers_lemma_pos</th>\n",
       "      <th>plausible_answers_lemma_pos</th>\n",
       "      <th>context_lemma_pos_tfidf</th>\n",
       "      <th>question_lemma_pos_tfidf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
       "      <td>When did Beyonce start becoming popular?</td>\n",
       "      <td>in the late 1990s</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>[(beyoncé, NN), (giselle, NN), (knowlescarter,...</td>\n",
       "      <td>[(when, WRB), (beyonce, NN), (start, NN), (bec...</td>\n",
       "      <td>[(late, RB), (1990s, NNS)]</td>\n",
       "      <td></td>\n",
       "      <td>[(beyoncé, NN, 0.013333333333333334), (giselle...</td>\n",
       "      <td>[(when, WRB, 0.2), (beyonce, NN, 0.2), (start,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
       "      <td>What areas did Beyonce compete in when she was...</td>\n",
       "      <td>singing and dancing</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>[(beyoncé, NN), (giselle, NN), (knowlescarter,...</td>\n",
       "      <td>[(what, WP), (area, NNS), (beyonce, VBP), (com...</td>\n",
       "      <td>[(sing, VBG), (dance, VBG)]</td>\n",
       "      <td></td>\n",
       "      <td>[(beyoncé, NN, 0.013333333333333334), (giselle...</td>\n",
       "      <td>[(what, WP, 0.2), (area, NNS, 0.2), (beyonce, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
       "      <td>When did Beyonce leave Destiny's Child and bec...</td>\n",
       "      <td>2003</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>[(beyoncé, NN), (giselle, NN), (knowlescarter,...</td>\n",
       "      <td>[(when, WRB), (beyonce, NN), (leave, VBP), (de...</td>\n",
       "      <td>[(2003, CD)]</td>\n",
       "      <td></td>\n",
       "      <td>[(beyoncé, NN, 0.013333333333333334), (giselle...</td>\n",
       "      <td>[(when, WRB, 0.125), (beyonce, NN, 0.125), (le...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
       "      <td>In what city and state did Beyonce  grow up?</td>\n",
       "      <td>Houston, Texas</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>[(beyoncé, NN), (giselle, NN), (knowlescarter,...</td>\n",
       "      <td>[(in, IN), (city, NN), (state, NN), (beyonce, ...</td>\n",
       "      <td>[(houston, NN), (texas, NN)]</td>\n",
       "      <td></td>\n",
       "      <td>[(beyoncé, NN, 0.013333333333333334), (giselle...</td>\n",
       "      <td>[(in, IN, 0.2), (city, NN, 0.2), (state, NN, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
       "      <td>In which decade did Beyonce become famous?</td>\n",
       "      <td>late 1990s</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>[(beyoncé, NN), (giselle, NN), (knowlescarter,...</td>\n",
       "      <td>[(in, IN), (decade, NN), (beyonce, NN), (becom...</td>\n",
       "      <td>[(late, RB), (1990s, NNS)]</td>\n",
       "      <td></td>\n",
       "      <td>[(beyoncé, NN, 0.013333333333333334), (giselle...</td>\n",
       "      <td>[(in, IN, 0.2), (decade, NN, 0.2), (beyonce, N...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             context  \\\n",
       "0  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...   \n",
       "1  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...   \n",
       "2  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...   \n",
       "3  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...   \n",
       "4  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...   \n",
       "\n",
       "                                            question              answers  \\\n",
       "0           When did Beyonce start becoming popular?    in the late 1990s   \n",
       "1  What areas did Beyonce compete in when she was...  singing and dancing   \n",
       "2  When did Beyonce leave Destiny's Child and bec...                 2003   \n",
       "3      In what city and state did Beyonce  grow up?        Houston, Texas   \n",
       "4         In which decade did Beyonce become famous?           late 1990s   \n",
       "\n",
       "  plausible_answers  is_impossible  \\\n",
       "0                                0   \n",
       "1                                0   \n",
       "2                                0   \n",
       "3                                0   \n",
       "4                                0   \n",
       "\n",
       "                                   context_lemma_pos  \\\n",
       "0  [(beyoncé, NN), (giselle, NN), (knowlescarter,...   \n",
       "1  [(beyoncé, NN), (giselle, NN), (knowlescarter,...   \n",
       "2  [(beyoncé, NN), (giselle, NN), (knowlescarter,...   \n",
       "3  [(beyoncé, NN), (giselle, NN), (knowlescarter,...   \n",
       "4  [(beyoncé, NN), (giselle, NN), (knowlescarter,...   \n",
       "\n",
       "                                  question_lemma_pos  \\\n",
       "0  [(when, WRB), (beyonce, NN), (start, NN), (bec...   \n",
       "1  [(what, WP), (area, NNS), (beyonce, VBP), (com...   \n",
       "2  [(when, WRB), (beyonce, NN), (leave, VBP), (de...   \n",
       "3  [(in, IN), (city, NN), (state, NN), (beyonce, ...   \n",
       "4  [(in, IN), (decade, NN), (beyonce, NN), (becom...   \n",
       "\n",
       "              answers_lemma_pos plausible_answers_lemma_pos  \\\n",
       "0    [(late, RB), (1990s, NNS)]                               \n",
       "1   [(sing, VBG), (dance, VBG)]                               \n",
       "2                  [(2003, CD)]                               \n",
       "3  [(houston, NN), (texas, NN)]                               \n",
       "4    [(late, RB), (1990s, NNS)]                               \n",
       "\n",
       "                             context_lemma_pos_tfidf  \\\n",
       "0  [(beyoncé, NN, 0.013333333333333334), (giselle...   \n",
       "1  [(beyoncé, NN, 0.013333333333333334), (giselle...   \n",
       "2  [(beyoncé, NN, 0.013333333333333334), (giselle...   \n",
       "3  [(beyoncé, NN, 0.013333333333333334), (giselle...   \n",
       "4  [(beyoncé, NN, 0.013333333333333334), (giselle...   \n",
       "\n",
       "                            question_lemma_pos_tfidf  \n",
       "0  [(when, WRB, 0.2), (beyonce, NN, 0.2), (start,...  \n",
       "1  [(what, WP, 0.2), (area, NNS, 0.2), (beyonce, ...  \n",
       "2  [(when, WRB, 0.125), (beyonce, NN, 0.125), (le...  \n",
       "3  [(in, IN, 0.2), (city, NN, 0.2), (state, NN, 0...  \n",
       "4  [(in, IN, 0.2), (decade, NN, 0.2), (beyonce, N...  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df.context_lemma_pos_tfidf[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/flatironschool/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "voc = lib.get_total_vocab(data=df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96682"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(voc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_vectors = lib.get_glove_vectors(filepath='glove.6B/glove.6B.300d.txt',\n",
    "                                     data=df, vocab=voc, \n",
    "                                     columns = ['context_lemma_pos', 'question_lemma_pos'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.6607e-01,  3.7760e-01, -3.7297e-01,  3.9632e-01, -2.5198e-01,\n",
       "        2.3823e-01, -5.1209e-01,  1.1211e-01, -2.3773e-01, -3.7428e-01,\n",
       "       -2.1506e-01, -3.7493e-01, -5.0230e-01,  1.2763e-01,  6.6347e-01,\n",
       "        3.1148e-01,  9.7449e-02, -4.6298e-02, -2.1342e-01, -9.8251e-03,\n",
       "        4.7481e-01, -2.5973e-01, -7.3478e-02,  1.8440e-01,  7.5455e-02,\n",
       "       -6.2966e-02,  2.6432e-01, -5.7984e-01, -2.3107e-01,  2.1037e-01,\n",
       "       -6.3662e-02, -1.4472e-01,  9.4807e-02,  2.3767e-02, -4.4796e-01,\n",
       "        1.2828e-01,  2.6972e-01, -7.0229e-02,  3.1668e-01, -4.0475e-01,\n",
       "       -5.6774e-02,  7.0430e-02,  7.4560e-01,  2.6541e-01,  2.7807e-03,\n",
       "       -1.3609e-01,  1.0039e+00, -1.3536e-01,  1.1548e-01, -2.8822e-01,\n",
       "        5.1668e-01,  4.1236e-02,  1.5980e-01,  1.9944e-01,  4.6439e-02,\n",
       "        1.6194e-01,  1.1778e-01, -2.5286e-01,  3.1291e-01, -3.1275e-01,\n",
       "        3.8737e-01,  6.3415e-02, -1.5682e-01,  4.3968e-01, -2.4797e-02,\n",
       "       -2.8540e-01,  9.8685e-03,  7.3803e-01, -1.5455e-01,  6.4833e-01,\n",
       "       -2.0055e-01,  3.8835e-01, -3.5942e-01,  7.7909e-01, -1.9399e-01,\n",
       "       -1.4242e-01,  1.9908e-01, -1.7022e-01,  3.2579e-01,  6.9043e-02,\n",
       "        4.3443e-02, -6.5667e-02, -1.4511e-01, -6.3334e-01, -2.4173e-01,\n",
       "       -6.0801e-01,  7.9222e-01,  3.2930e-01, -8.3806e-02, -1.7304e-01,\n",
       "       -2.1610e-01,  4.0626e-01,  2.8471e-01, -3.6579e-01,  5.4183e-01,\n",
       "        8.2396e-01, -1.3860e-01,  2.2070e-01, -1.1346e+00, -2.7535e-01,\n",
       "        4.8256e-01,  3.9510e-01,  4.6202e-01,  1.4330e-02,  4.4202e-01,\n",
       "        5.1190e-01, -2.4710e-01, -4.5378e-01,  6.0677e-01,  7.3485e-01,\n",
       "        2.4519e-01,  5.8714e-01, -1.6256e-01,  2.8133e-01,  5.0748e-01,\n",
       "        4.8580e-02, -5.0343e-01,  2.9946e-01,  1.5827e-01, -5.7623e-01,\n",
       "       -2.2915e-01,  1.4257e-01,  4.7336e-01,  8.2119e-02, -3.5748e-01,\n",
       "        2.5803e-01, -1.7411e-01,  2.6486e-01,  4.1482e-01,  2.4794e-01,\n",
       "        1.8071e-02, -8.3274e-01,  2.8878e-01,  8.0838e-01,  8.9202e-03,\n",
       "        4.3080e-02, -3.6419e-01,  3.0611e-01, -3.7256e-01,  2.6607e-01,\n",
       "        1.4095e-01, -1.7304e-01, -1.6555e-01, -9.6376e-01,  6.2513e-01,\n",
       "       -4.2336e-02,  6.0229e-01,  5.9044e-01, -1.6869e-01, -1.4139e-01,\n",
       "        4.4721e-01, -2.7164e-02,  1.8082e-01,  5.3584e-01,  2.9029e-02,\n",
       "       -6.1316e-01,  8.1919e-02,  4.9692e-01, -2.2852e-01, -3.1796e-01,\n",
       "        2.5085e-01,  3.0988e-01,  1.4000e-01,  7.0781e-01, -5.1576e-01,\n",
       "        4.5491e-01, -2.1083e-01, -1.1888e-01, -4.6364e-01,  4.4421e-01,\n",
       "       -1.2584e-01,  6.8121e-01,  1.4559e-01, -3.0980e-01,  2.7984e-01,\n",
       "        4.3562e-01,  9.4599e-02,  1.3148e-02,  6.6309e-01,  1.4266e-01,\n",
       "        4.1878e-01, -6.9720e-01, -8.9929e-01,  1.8981e-01, -8.8519e-01,\n",
       "       -2.1505e-01,  8.2612e-01, -1.9075e-02,  2.4058e-01, -4.6339e-01,\n",
       "        5.4448e-02,  5.4431e-01, -2.0949e-01, -5.8443e-01, -2.8400e-01,\n",
       "        4.5387e-01,  6.2400e-01, -5.2498e-01, -1.0185e-01, -1.5322e-01,\n",
       "        7.2829e-01,  2.6487e-01, -7.8573e-01, -5.5756e-01,  1.6281e-01,\n",
       "       -5.9319e-02, -9.7344e-01, -3.9000e-01,  1.1144e+00,  2.1973e-01,\n",
       "       -2.0238e-01, -4.8492e-01,  3.0114e-01, -5.1838e-02,  3.5524e-01,\n",
       "        1.6588e-01, -2.0831e-01,  6.2509e-01,  7.8385e-01, -2.8395e-01,\n",
       "        5.0937e-02, -9.4872e-02, -6.0582e-01,  9.6501e-01, -1.3112e-01,\n",
       "        3.3989e-01,  2.0663e-01,  3.0488e-01,  6.6848e-02, -1.7964e-01,\n",
       "        3.8561e-01,  1.4942e-01,  1.4090e-01, -7.7108e-02,  3.9290e-01,\n",
       "        6.6950e-01,  5.1302e-01, -5.2391e-01,  1.3876e-01, -1.8030e-01,\n",
       "        4.0609e-02, -2.0153e-03, -4.9575e-01,  8.6265e-01, -1.0458e+00,\n",
       "        2.9279e-01, -8.3324e-02,  1.2205e-01,  3.5002e-01, -2.0719e-01,\n",
       "        2.0739e-01,  5.6732e-01,  1.0219e-01,  2.7285e-01,  9.8154e-01,\n",
       "       -6.5213e-01,  1.1916e+00, -8.1439e-01,  9.8854e-04, -8.0588e-01,\n",
       "       -4.7273e-01,  2.8966e-01,  1.4759e-01, -1.2248e-01,  1.3859e-01,\n",
       "        3.5534e-01,  2.9032e-01, -8.5889e-01, -1.5329e-01,  1.5683e-01,\n",
       "       -5.3241e-01,  1.1347e-01,  1.1751e-01,  5.0287e-02,  4.3739e-01,\n",
       "       -1.8908e-01,  2.2310e-01,  3.2318e-01, -1.0573e+00, -1.8291e-01,\n",
       "       -5.2374e-01,  3.1093e-01, -2.6702e-01,  4.4724e-01, -4.9211e-01,\n",
       "        4.5807e-01,  1.5022e-01, -1.7001e-01,  2.5377e-01, -9.8446e-02,\n",
       "        1.0472e+00, -4.6087e-01,  2.2306e-01,  5.9727e-01,  1.8822e-01,\n",
       "       -5.2261e-02,  2.6815e-01,  1.2029e-01, -5.3737e-02,  5.8141e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_vectors['bee']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_df = df.drop(columns = ['answers', 'plausible_answers', 'answers_lemma_pos', 'plausible_answers_lemma_pos'])\n",
    "y_df = df.drop(columns = x_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x_train_df, x_test_df, y_train_df, y_test_df = train_test_split(x_df,y_df, random_state=43, test_size=.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "      <th>is_impossible</th>\n",
       "      <th>context_lemma_pos</th>\n",
       "      <th>question_lemma_pos</th>\n",
       "      <th>context_lemma_pos_tfidf</th>\n",
       "      <th>question_lemma_pos_tfidf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>35207</td>\n",
       "      <td>India started the construction of a 40,000-ton...</td>\n",
       "      <td>Who isn't constructing the Indian-made ship?</td>\n",
       "      <td>1</td>\n",
       "      <td>[(india, NN), (start, VBD), (construction, NN)...</td>\n",
       "      <td>[(who, WP), (isnt, VBP), (construct, VBG), (in...</td>\n",
       "      <td>[(india, NN, 0.017543859649122806), (start, VB...</td>\n",
       "      <td>[(who, WP, 0.2), (isnt, VBP, 0.2), (construct,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16433</td>\n",
       "      <td>The investigation of the transmitted signals s...</td>\n",
       "      <td>Knowing the codes allowed Stanford researchers...</td>\n",
       "      <td>1</td>\n",
       "      <td>[(the, DT), (investigation, NN), (transmit, VB...</td>\n",
       "      <td>[(know, VBG), (code, NNS), (allow, VBN), (stan...</td>\n",
       "      <td>[(the, DT, 0.0392156862745098), (investigation...</td>\n",
       "      <td>[(know, VBG, 0.1111111111111111), (code, NNS, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22453</td>\n",
       "      <td>Note: Besides ethnic groups, Slavs often ident...</td>\n",
       "      <td>Where are Vlaji located?</td>\n",
       "      <td>0</td>\n",
       "      <td>[(note, NN), (besides, IN), (ethnic, JJ), (gro...</td>\n",
       "      <td>[(where, WRB), (vlaji, NN), (locate, VBD)]</td>\n",
       "      <td>[(note, NN, 0.008620689655172414), (besides, I...</td>\n",
       "      <td>[(where, WRB, 0.3333333333333333), (vlaji, NN,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63333</td>\n",
       "      <td>The ceremonial county of Somerset consists of ...</td>\n",
       "      <td>For how many years did Avon county exist?</td>\n",
       "      <td>1</td>\n",
       "      <td>[(the, DT), (ceremonial, JJ), (county, NN), (s...</td>\n",
       "      <td>[(for, IN), (many, JJ), (year, NNS), (avon, VB...</td>\n",
       "      <td>[(the, DT, 0.05172413793103448), (ceremonial, ...</td>\n",
       "      <td>[(for, IN, 0.16666666666666666), (many, JJ, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>104786</td>\n",
       "      <td>Initially the change in strategy caught the RA...</td>\n",
       "      <td>The bombing of the Thames Estuary cause how ma...</td>\n",
       "      <td>0</td>\n",
       "      <td>[(initially, RB), (change, VBP), (strategy, NN...</td>\n",
       "      <td>[(the, DT), (bombing, NN), (thames, NNS), (est...</td>\n",
       "      <td>[(initially, RB, 0.011494252873563218), (chang...</td>\n",
       "      <td>[(the, DT, 0.125), (bombing, NN, 0.125), (tham...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  context  \\\n",
       "35207   India started the construction of a 40,000-ton...   \n",
       "16433   The investigation of the transmitted signals s...   \n",
       "22453   Note: Besides ethnic groups, Slavs often ident...   \n",
       "63333   The ceremonial county of Somerset consists of ...   \n",
       "104786  Initially the change in strategy caught the RA...   \n",
       "\n",
       "                                                 question  is_impossible  \\\n",
       "35207        Who isn't constructing the Indian-made ship?              1   \n",
       "16433   Knowing the codes allowed Stanford researchers...              1   \n",
       "22453                            Where are Vlaji located?              0   \n",
       "63333           For how many years did Avon county exist?              1   \n",
       "104786  The bombing of the Thames Estuary cause how ma...              0   \n",
       "\n",
       "                                        context_lemma_pos  \\\n",
       "35207   [(india, NN), (start, VBD), (construction, NN)...   \n",
       "16433   [(the, DT), (investigation, NN), (transmit, VB...   \n",
       "22453   [(note, NN), (besides, IN), (ethnic, JJ), (gro...   \n",
       "63333   [(the, DT), (ceremonial, JJ), (county, NN), (s...   \n",
       "104786  [(initially, RB), (change, VBP), (strategy, NN...   \n",
       "\n",
       "                                       question_lemma_pos  \\\n",
       "35207   [(who, WP), (isnt, VBP), (construct, VBG), (in...   \n",
       "16433   [(know, VBG), (code, NNS), (allow, VBN), (stan...   \n",
       "22453          [(where, WRB), (vlaji, NN), (locate, VBD)]   \n",
       "63333   [(for, IN), (many, JJ), (year, NNS), (avon, VB...   \n",
       "104786  [(the, DT), (bombing, NN), (thames, NNS), (est...   \n",
       "\n",
       "                                  context_lemma_pos_tfidf  \\\n",
       "35207   [(india, NN, 0.017543859649122806), (start, VB...   \n",
       "16433   [(the, DT, 0.0392156862745098), (investigation...   \n",
       "22453   [(note, NN, 0.008620689655172414), (besides, I...   \n",
       "63333   [(the, DT, 0.05172413793103448), (ceremonial, ...   \n",
       "104786  [(initially, RB, 0.011494252873563218), (chang...   \n",
       "\n",
       "                                 question_lemma_pos_tfidf  \n",
       "35207   [(who, WP, 0.2), (isnt, VBP, 0.2), (construct,...  \n",
       "16433   [(know, VBG, 0.1111111111111111), (code, NNS, ...  \n",
       "22453   [(where, WRB, 0.3333333333333333), (vlaji, NN,...  \n",
       "63333   [(for, IN, 0.16666666666666666), (many, JJ, 0....  \n",
       "104786  [(the, DT, 0.125), (bombing, NN, 0.125), (tham...  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('where', 'WRB'), ('vlaji', 'NN'), ('locate', 'VBD')]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_df.question_lemma_pos[22453]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
