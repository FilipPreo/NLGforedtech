{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/flatironschool/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# testing lib fn\n",
    "from __future__ import print_function, unicode_literals\n",
    "from unicodedata import normalize\n",
    "import library_py2 as lib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "# from sklearn.metrics import roc\n",
    "# from sklearn.model_selection import traintestsplit\n",
    "# from sklearn.preprocessing import \n",
    "import json\n",
    "import warnings\n",
    "# from nltk.punkt impor\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "import string\n",
    "from nltk.stem.wordnet import WordNetLemmatizer, wordnet\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.collocations import *\n",
    "from nltk import FreqDist\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk import word_tokenize\n",
    "import re\n",
    "import os\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "warnings.filterwarnings(action='once')\n",
    "%matplotlib inline\n",
    "pd.set_option('display.max_columns', 999)\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Input, Dense, LSTM, Embedding, Masking\n",
    "from keras.layers import Dropout, Activation, Bidirectional, GlobalMaxPool1D\n",
    "from keras.models import Model\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import os\n",
    "import argparse\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Activation, Embedding, Dropout, TimeDistributed\n",
    "from keras.layers import LSTM\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('df1_pos_W.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_dict = lib.get_pos_dict(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = lib.concat_word_pos(df, columns=['context_lemma_pos', 'question_lemma_pos','answers_lemma_pos'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vocabulary = lib.get_total_vocab(df, columns=['context_lemma_pos', 'question_lemma_pos'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['context_lemma_pos'] + ' ' + df['question_lemma_pos']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['question_answer'] = df['question_lemma_pos'] + ' ' + df['answers_lemma_pos']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context_lemma_pos</th>\n",
       "      <th>question_lemma_pos</th>\n",
       "      <th>answers_lemma_pos</th>\n",
       "      <th>answer_len</th>\n",
       "      <th>question_len</th>\n",
       "      <th>context_len</th>\n",
       "      <th>text</th>\n",
       "      <th>question_answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>beyonc_NN giselle_NN knowlescarter_NN bijnse_N...</td>\n",
       "      <td>when_WRB beyonce_NN leave_VBP destinys_VBN chi...</td>\n",
       "      <td>2003_CD</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>75</td>\n",
       "      <td>beyonc_NN giselle_NN knowlescarter_NN bijnse_N...</td>\n",
       "      <td>when_WRB beyonce_NN leave_VBP destinys_VBN chi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>beyonc_NN giselle_NN knowlescarter_NN bijnse_N...</td>\n",
       "      <td>when_WRB beyonc_NN release_NN dangerously_RB l...</td>\n",
       "      <td>2003_CD</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>75</td>\n",
       "      <td>beyonc_NN giselle_NN knowlescarter_NN bijnse_N...</td>\n",
       "      <td>when_WRB beyonc_NN release_NN dangerously_RB l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>beyonc_NN giselle_NN knowlescarter_NN bijnse_N...</td>\n",
       "      <td>how_WRB many_JJ grammy_JJ award_NNS beyonc_NN ...</td>\n",
       "      <td>five_CD</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>75</td>\n",
       "      <td>beyonc_NN giselle_NN knowlescarter_NN bijnse_N...</td>\n",
       "      <td>how_WRB many_JJ grammy_JJ award_NNS beyonc_NN ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>follow_VBG disbandment_NN destiny_NN child_NN ...</td>\n",
       "      <td>after_IN second_JJ solo_NN album_NN entertainm...</td>\n",
       "      <td>act_VBG</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>112</td>\n",
       "      <td>follow_VBG disbandment_NN destiny_NN child_NN ...</td>\n",
       "      <td>after_IN second_JJ solo_NN album_NN entertainm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>follow_VBG disbandment_NN destiny_NN child_NN ...</td>\n",
       "      <td>to_TO set_VB record_NN grammys_NN many_JJ beyo...</td>\n",
       "      <td>six_CD</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>112</td>\n",
       "      <td>follow_VBG disbandment_NN destiny_NN child_NN ...</td>\n",
       "      <td>to_TO set_VB record_NN grammys_NN many_JJ beyo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    context_lemma_pos  \\\n",
       "2   beyonc_NN giselle_NN knowlescarter_NN bijnse_N...   \n",
       "11  beyonc_NN giselle_NN knowlescarter_NN bijnse_N...   \n",
       "12  beyonc_NN giselle_NN knowlescarter_NN bijnse_N...   \n",
       "15  follow_VBG disbandment_NN destiny_NN child_NN ...   \n",
       "17  follow_VBG disbandment_NN destiny_NN child_NN ...   \n",
       "\n",
       "                                   question_lemma_pos answers_lemma_pos  \\\n",
       "2   when_WRB beyonce_NN leave_VBP destinys_VBN chi...           2003_CD   \n",
       "11  when_WRB beyonc_NN release_NN dangerously_RB l...           2003_CD   \n",
       "12  how_WRB many_JJ grammy_JJ award_NNS beyonc_NN ...           five_CD   \n",
       "15  after_IN second_JJ solo_NN album_NN entertainm...           act_VBG   \n",
       "17  to_TO set_VB record_NN grammys_NN many_JJ beyo...            six_CD   \n",
       "\n",
       "    answer_len  question_len  context_len  \\\n",
       "2            1             8           75   \n",
       "11           1             5           75   \n",
       "12           1             9           75   \n",
       "15           1             8          112   \n",
       "17           1             7          112   \n",
       "\n",
       "                                                 text  \\\n",
       "2   beyonc_NN giselle_NN knowlescarter_NN bijnse_N...   \n",
       "11  beyonc_NN giselle_NN knowlescarter_NN bijnse_N...   \n",
       "12  beyonc_NN giselle_NN knowlescarter_NN bijnse_N...   \n",
       "15  follow_VBG disbandment_NN destiny_NN child_NN ...   \n",
       "17  follow_VBG disbandment_NN destiny_NN child_NN ...   \n",
       "\n",
       "                                      question_answer  \n",
       "2   when_WRB beyonce_NN leave_VBP destinys_VBN chi...  \n",
       "11  when_WRB beyonc_NN release_NN dangerously_RB l...  \n",
       "12  how_WRB many_JJ grammy_JJ award_NNS beyonc_NN ...  \n",
       "15  after_IN second_JJ solo_NN album_NN entertainm...  \n",
       "17  to_TO set_VB record_NN grammys_NN many_JJ beyo...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First five words  in vectorized format :  [113534, 33057, 13160, 73117, 74266]\n",
      "Vocabulary examples : \n",
      "1.  cimas_VBP 55621\n",
      "2.  nbl_JJ 29466\n",
      "3.  konishi_NN 37825\n",
      " Size of vocabulary :  127983\n",
      "beyonc_NN giselle_NN knowlescarter_NN bijnse_NN beeyonsay_VBP born_JJ september_NN 4_CD 1981_CD american_JJ singer_NN songwriter_NN record_NN producer_NN actress_NN born_NNS raise_VBD houston_NN texas_NN perform_VBD various_JJ sing_VBG dance_VBG competition_NNS child_NN rise_VBD fame_JJ late_RB 1990s_CD lead_JJ\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_data, valid_data, vocabulary, reversed_dictionary, vocabulary_size, embedding_matrix, embedding_dim = lib.load_sequential_data(df,\n",
    "                                                                                                                                    pos_dict,\n",
    "                                                                                                                                    shuffle_df=False,\n",
    "                                                                                                                                    use_glove=True, \n",
    "                                                                                                                                    embedding_dim=300, \n",
    "                                                                                                                                    use_pos=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "301"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "127983"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "952856"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context_lemma_pos</th>\n",
       "      <th>question_lemma_pos</th>\n",
       "      <th>answers_lemma_pos</th>\n",
       "      <th>answer_len</th>\n",
       "      <th>question_len</th>\n",
       "      <th>context_len</th>\n",
       "      <th>text</th>\n",
       "      <th>question_answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>beyonc_NN giselle_NN knowlescarter_NN bijnse_N...</td>\n",
       "      <td>when_WRB beyonce_NN leave_VBP destinys_VBN chi...</td>\n",
       "      <td>2003_CD</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>75</td>\n",
       "      <td>beyonc_NN giselle_NN knowlescarter_NN bijnse_N...</td>\n",
       "      <td>when_WRB beyonce_NN leave_VBP destinys_VBN chi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>beyonc_NN giselle_NN knowlescarter_NN bijnse_N...</td>\n",
       "      <td>when_WRB beyonc_NN release_NN dangerously_RB l...</td>\n",
       "      <td>2003_CD</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>75</td>\n",
       "      <td>beyonc_NN giselle_NN knowlescarter_NN bijnse_N...</td>\n",
       "      <td>when_WRB beyonc_NN release_NN dangerously_RB l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>beyonc_NN giselle_NN knowlescarter_NN bijnse_N...</td>\n",
       "      <td>how_WRB many_JJ grammy_JJ award_NNS beyonc_NN ...</td>\n",
       "      <td>five_CD</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>75</td>\n",
       "      <td>beyonc_NN giselle_NN knowlescarter_NN bijnse_N...</td>\n",
       "      <td>how_WRB many_JJ grammy_JJ award_NNS beyonc_NN ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>follow_VBG disbandment_NN destiny_NN child_NN ...</td>\n",
       "      <td>after_IN second_JJ solo_NN album_NN entertainm...</td>\n",
       "      <td>act_VBG</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>112</td>\n",
       "      <td>follow_VBG disbandment_NN destiny_NN child_NN ...</td>\n",
       "      <td>after_IN second_JJ solo_NN album_NN entertainm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>follow_VBG disbandment_NN destiny_NN child_NN ...</td>\n",
       "      <td>to_TO set_VB record_NN grammys_NN many_JJ beyo...</td>\n",
       "      <td>six_CD</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>112</td>\n",
       "      <td>follow_VBG disbandment_NN destiny_NN child_NN ...</td>\n",
       "      <td>to_TO set_VB record_NN grammys_NN many_JJ beyo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   context_lemma_pos  \\\n",
       "0  beyonc_NN giselle_NN knowlescarter_NN bijnse_N...   \n",
       "1  beyonc_NN giselle_NN knowlescarter_NN bijnse_N...   \n",
       "2  beyonc_NN giselle_NN knowlescarter_NN bijnse_N...   \n",
       "3  follow_VBG disbandment_NN destiny_NN child_NN ...   \n",
       "4  follow_VBG disbandment_NN destiny_NN child_NN ...   \n",
       "\n",
       "                                  question_lemma_pos answers_lemma_pos  \\\n",
       "0  when_WRB beyonce_NN leave_VBP destinys_VBN chi...           2003_CD   \n",
       "1  when_WRB beyonc_NN release_NN dangerously_RB l...           2003_CD   \n",
       "2  how_WRB many_JJ grammy_JJ award_NNS beyonc_NN ...           five_CD   \n",
       "3  after_IN second_JJ solo_NN album_NN entertainm...           act_VBG   \n",
       "4  to_TO set_VB record_NN grammys_NN many_JJ beyo...            six_CD   \n",
       "\n",
       "   answer_len  question_len  context_len  \\\n",
       "0           1             8           75   \n",
       "1           1             5           75   \n",
       "2           1             9           75   \n",
       "3           1             8          112   \n",
       "4           1             7          112   \n",
       "\n",
       "                                                text  \\\n",
       "0  beyonc_NN giselle_NN knowlescarter_NN bijnse_N...   \n",
       "1  beyonc_NN giselle_NN knowlescarter_NN bijnse_N...   \n",
       "2  beyonc_NN giselle_NN knowlescarter_NN bijnse_N...   \n",
       "3  follow_VBG disbandment_NN destiny_NN child_NN ...   \n",
       "4  follow_VBG disbandment_NN destiny_NN child_NN ...   \n",
       "\n",
       "                                     question_answer  \n",
       "0  when_WRB beyonce_NN leave_VBP destinys_VBN chi...  \n",
       "1  when_WRB beyonc_NN release_NN dangerously_RB l...  \n",
       "2  how_WRB many_JJ grammy_JJ award_NNS beyonc_NN ...  \n",
       "3  after_IN second_JJ solo_NN album_NN entertainm...  \n",
       "4  to_TO set_VB record_NN grammys_NN many_JJ beyo...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=20\n",
    "num_epochs=20\n",
    "num_steps = 4\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_generator = lib.KerasBatchGenerator(train_data, num_steps=num_steps,\n",
    "                                              batch_size=batch_size,\n",
    "                                              vocabulary=vocabulary, \n",
    "                                              skip_step=df.question_len.max()+1)\n",
    "valid_data_generator = lib.KerasBatchGenerator(valid_data, num_steps=num_steps,\n",
    "                                              batch_size=batch_size,\n",
    "                                              vocabulary=vocabulary, \n",
    "                                              skip_step=df.question_len.max()+1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.question_len.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 4, 301)            38522883  \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 4, 301)            726012    \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 4, 301)            726012    \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                (None, 4, 301)            726012    \n",
      "_________________________________________________________________\n",
      "time_distributed_2 (TimeDist (None, 4, 127983)         38650866  \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 4, 127983)         0         \n",
      "=================================================================\n",
      "Total params: 79,351,785\n",
      "Trainable params: 79,351,785\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "hidden_size = embedding_dim\n",
    "use_dropout=True\n",
    "num_epochs=20\n",
    "\n",
    "model_seq = Sequential([\n",
    "    layers.Embedding(input_dim=vocabulary_size, \n",
    "                     output_dim=hidden_size,input_length=num_steps,\n",
    "                     weights = [embedding_matrix],\n",
    "                     mask_zero=True, trainable=True),\n",
    "    layers.LSTM(hidden_size, return_sequences=True),\n",
    "    layers.LSTM(hidden_size, return_sequences=True),\n",
    "    layers.LSTM(hidden_size, dropout=0.25, return_sequences=True),\n",
    "    layers.TimeDistributed(Dense(vocabulary_size)),\n",
    "    layers.Activation('softmax')\n",
    "])\n",
    "\n",
    "\n",
    "optimizer=Adam()\n",
    "model_seq.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['categorical_accuracy'])\n",
    "\n",
    "print(model_seq.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/120\n",
      "30/30 [==============================] - 94s 3s/step - loss: 11.6525 - categorical_accuracy: 0.0133 - val_loss: 11.5450 - val_categorical_accuracy: 0.0075\n",
      "Epoch 2/120\n",
      "30/30 [==============================] - 88s 3s/step - loss: 11.6273 - categorical_accuracy: 0.0042 - val_loss: 12.5779 - val_categorical_accuracy: 0.0000e+00\n",
      "Epoch 3/120\n",
      "30/30 [==============================] - 88s 3s/step - loss: 11.7253 - categorical_accuracy: 0.0042 - val_loss: 11.0407 - val_categorical_accuracy: 0.0075\n",
      "Epoch 4/120\n",
      "30/30 [==============================] - 88s 3s/step - loss: 11.0059 - categorical_accuracy: 0.0083 - val_loss: 10.8665 - val_categorical_accuracy: 0.0200\n",
      "Epoch 5/120\n",
      "30/30 [==============================] - 90s 3s/step - loss: 10.2254 - categorical_accuracy: 0.0308 - val_loss: 11.2516 - val_categorical_accuracy: 0.0013\n",
      "Epoch 6/120\n",
      "30/30 [==============================] - 87s 3s/step - loss: 11.0087 - categorical_accuracy: 0.0042 - val_loss: 10.9824 - val_categorical_accuracy: 0.0125\n",
      "Epoch 7/120\n",
      "30/30 [==============================] - 87s 3s/step - loss: 10.7432 - categorical_accuracy: 0.0067 - val_loss: 10.7233 - val_categorical_accuracy: 0.0050\n",
      "Epoch 8/120\n",
      "30/30 [==============================] - 87s 3s/step - loss: 10.3971 - categorical_accuracy: 0.0050 - val_loss: 11.0794 - val_categorical_accuracy: 0.0013\n",
      "Epoch 9/120\n",
      "30/30 [==============================] - 88s 3s/step - loss: 10.5512 - categorical_accuracy: 0.0067 - val_loss: 10.7226 - val_categorical_accuracy: 0.0025\n",
      "Epoch 10/120\n",
      "30/30 [==============================] - 88s 3s/step - loss: 10.3367 - categorical_accuracy: 0.0167 - val_loss: 10.7656 - val_categorical_accuracy: 0.0113\n",
      "Epoch 11/120\n",
      "30/30 [==============================] - 88s 3s/step - loss: 10.3736 - categorical_accuracy: 0.0100 - val_loss: 10.3877 - val_categorical_accuracy: 0.0075\n",
      "Epoch 12/120\n",
      "30/30 [==============================] - 88s 3s/step - loss: 10.4184 - categorical_accuracy: 0.0083 - val_loss: 10.2427 - val_categorical_accuracy: 0.0125\n",
      "Epoch 13/120\n",
      "30/30 [==============================] - 88s 3s/step - loss: 10.1306 - categorical_accuracy: 0.0117 - val_loss: 10.3141 - val_categorical_accuracy: 0.0088\n",
      "Epoch 14/120\n",
      "30/30 [==============================] - 87s 3s/step - loss: 10.1661 - categorical_accuracy: 0.0150 - val_loss: 10.2116 - val_categorical_accuracy: 0.0200\n",
      "Epoch 15/120\n",
      "30/30 [==============================] - 85s 3s/step - loss: 10.3532 - categorical_accuracy: 0.0158 - val_loss: 10.4843 - val_categorical_accuracy: 0.0138\n",
      "Epoch 16/120\n",
      "30/30 [==============================] - 85s 3s/step - loss: 10.2847 - categorical_accuracy: 0.0092 - val_loss: 10.2711 - val_categorical_accuracy: 0.0213\n",
      "Epoch 17/120\n",
      "30/30 [==============================] - 85s 3s/step - loss: 10.3843 - categorical_accuracy: 0.0125 - val_loss: 10.2216 - val_categorical_accuracy: 0.0113\n",
      "Epoch 18/120\n",
      "30/30 [==============================] - 85s 3s/step - loss: 10.3152 - categorical_accuracy: 0.0167 - val_loss: 10.1206 - val_categorical_accuracy: 0.0175\n",
      "Epoch 19/120\n",
      "30/30 [==============================] - 85s 3s/step - loss: 10.3279 - categorical_accuracy: 0.0183 - val_loss: 10.0981 - val_categorical_accuracy: 0.0213\n",
      "Epoch 20/120\n",
      "30/30 [==============================] - 85s 3s/step - loss: 10.0913 - categorical_accuracy: 0.0150 - val_loss: 9.9534 - val_categorical_accuracy: 0.0213\n",
      "Epoch 21/120\n",
      "30/30 [==============================] - 85s 3s/step - loss: 10.2455 - categorical_accuracy: 0.0125 - val_loss: 10.1278 - val_categorical_accuracy: 0.0150\n",
      "Epoch 22/120\n",
      "30/30 [==============================] - 87s 3s/step - loss: 9.8910 - categorical_accuracy: 0.0183 - val_loss: 10.3356 - val_categorical_accuracy: 0.0138\n",
      "Epoch 23/120\n",
      "30/30 [==============================] - 85s 3s/step - loss: 9.7835 - categorical_accuracy: 0.0125 - val_loss: 10.0509 - val_categorical_accuracy: 0.0150\n",
      "Epoch 24/120\n",
      "30/30 [==============================] - 85s 3s/step - loss: 10.2383 - categorical_accuracy: 0.0158 - val_loss: 10.3978 - val_categorical_accuracy: 0.0163\n",
      "Epoch 25/120\n",
      "30/30 [==============================] - 85s 3s/step - loss: 10.2024 - categorical_accuracy: 0.0208 - val_loss: 10.0765 - val_categorical_accuracy: 0.0150\n",
      "Epoch 26/120\n",
      "30/30 [==============================] - 85s 3s/step - loss: 9.9228 - categorical_accuracy: 0.0150 - val_loss: 10.3998 - val_categorical_accuracy: 0.0075\n",
      "Epoch 27/120\n",
      "30/30 [==============================] - 86s 3s/step - loss: 10.1885 - categorical_accuracy: 0.0117 - val_loss: 10.4370 - val_categorical_accuracy: 0.0138\n",
      "Epoch 28/120\n",
      "30/30 [==============================] - 85s 3s/step - loss: 10.4955 - categorical_accuracy: 0.0183 - val_loss: 10.1933 - val_categorical_accuracy: 0.0150\n",
      "Epoch 29/120\n",
      "30/30 [==============================] - 85s 3s/step - loss: 10.1719 - categorical_accuracy: 0.0200 - val_loss: 10.0279 - val_categorical_accuracy: 0.0188\n",
      "Epoch 30/120\n",
      "30/30 [==============================] - 85s 3s/step - loss: 9.8685 - categorical_accuracy: 0.0200 - val_loss: 10.0083 - val_categorical_accuracy: 0.0138\n",
      "Epoch 31/120\n",
      "30/30 [==============================] - 86s 3s/step - loss: 10.2884 - categorical_accuracy: 0.0117 - val_loss: 9.9753 - val_categorical_accuracy: 0.0200\n",
      "Epoch 32/120\n",
      "30/30 [==============================] - 85s 3s/step - loss: 10.0888 - categorical_accuracy: 0.0117 - val_loss: 10.1964 - val_categorical_accuracy: 0.0338\n",
      "Epoch 33/120\n",
      "30/30 [==============================] - 86s 3s/step - loss: 10.0741 - categorical_accuracy: 0.0142 - val_loss: 10.3262 - val_categorical_accuracy: 0.0138\n",
      "Epoch 34/120\n",
      "30/30 [==============================] - 85s 3s/step - loss: 9.8423 - categorical_accuracy: 0.0283 - val_loss: 10.5170 - val_categorical_accuracy: 0.0038\n",
      "Epoch 35/120\n",
      "30/30 [==============================] - 86s 3s/step - loss: 10.1049 - categorical_accuracy: 0.0192 - val_loss: 10.1165 - val_categorical_accuracy: 0.0138\n",
      "Epoch 36/120\n",
      "30/30 [==============================] - 86s 3s/step - loss: 10.0744 - categorical_accuracy: 0.0200 - val_loss: 10.0438 - val_categorical_accuracy: 0.0100\n",
      "Epoch 37/120\n",
      "30/30 [==============================] - 85s 3s/step - loss: 10.1348 - categorical_accuracy: 0.0175 - val_loss: 9.8276 - val_categorical_accuracy: 0.0125\n",
      "Epoch 38/120\n",
      "30/30 [==============================] - 85s 3s/step - loss: 10.1524 - categorical_accuracy: 0.0142 - val_loss: 10.2595 - val_categorical_accuracy: 0.0088\n",
      "Epoch 39/120\n",
      "30/30 [==============================] - 89s 3s/step - loss: 10.2014 - categorical_accuracy: 0.0142 - val_loss: 9.9452 - val_categorical_accuracy: 0.0213\n",
      "Epoch 40/120\n",
      "30/30 [==============================] - 88s 3s/step - loss: 9.9231 - categorical_accuracy: 0.0233 - val_loss: 10.2497 - val_categorical_accuracy: 0.0150\n",
      "Epoch 41/120\n",
      "30/30 [==============================] - 88s 3s/step - loss: 10.0070 - categorical_accuracy: 0.0167 - val_loss: 10.1305 - val_categorical_accuracy: 0.0188\n",
      "Epoch 42/120\n",
      "30/30 [==============================] - 88s 3s/step - loss: 10.0612 - categorical_accuracy: 0.0267 - val_loss: 10.0176 - val_categorical_accuracy: 0.0113\n",
      "Epoch 43/120\n",
      "30/30 [==============================] - 88s 3s/step - loss: 9.9827 - categorical_accuracy: 0.0167 - val_loss: 10.0340 - val_categorical_accuracy: 0.0200\n",
      "Epoch 44/120\n",
      "30/30 [==============================] - 88s 3s/step - loss: 10.1893 - categorical_accuracy: 0.0108 - val_loss: 9.9379 - val_categorical_accuracy: 0.0213\n",
      "Epoch 45/120\n",
      "30/30 [==============================] - 88s 3s/step - loss: 10.1426 - categorical_accuracy: 0.0208 - val_loss: 9.7662 - val_categorical_accuracy: 0.0238\n",
      "Epoch 46/120\n",
      "30/30 [==============================] - 88s 3s/step - loss: 10.2411 - categorical_accuracy: 0.0125 - val_loss: 9.9374 - val_categorical_accuracy: 0.0138\n",
      "Epoch 47/120\n",
      "30/30 [==============================] - 88s 3s/step - loss: 10.1370 - categorical_accuracy: 0.0100 - val_loss: 10.1879 - val_categorical_accuracy: 0.0175\n",
      "Epoch 48/120\n",
      "30/30 [==============================] - 88s 3s/step - loss: 10.1652 - categorical_accuracy: 0.0117 - val_loss: 9.8926 - val_categorical_accuracy: 0.0163\n",
      "Epoch 49/120\n",
      "30/30 [==============================] - 88s 3s/step - loss: 10.0449 - categorical_accuracy: 0.0200 - val_loss: 10.3188 - val_categorical_accuracy: 0.0088\n",
      "Epoch 50/120\n",
      "30/30 [==============================] - 88s 3s/step - loss: 9.8892 - categorical_accuracy: 0.0200 - val_loss: 10.1283 - val_categorical_accuracy: 0.0213\n",
      "Epoch 51/120\n",
      "30/30 [==============================] - 89s 3s/step - loss: 9.9412 - categorical_accuracy: 0.0092 - val_loss: 10.0026 - val_categorical_accuracy: 0.0075\n",
      "Epoch 52/120\n",
      "30/30 [==============================] - 88s 3s/step - loss: 10.4282 - categorical_accuracy: 0.0083 - val_loss: 10.2836 - val_categorical_accuracy: 0.0138\n",
      "Epoch 53/120\n",
      "30/30 [==============================] - 88s 3s/step - loss: 10.1635 - categorical_accuracy: 0.0142 - val_loss: 10.1632 - val_categorical_accuracy: 0.0150\n",
      "Epoch 54/120\n",
      "30/30 [==============================] - 88s 3s/step - loss: 10.0377 - categorical_accuracy: 0.0158 - val_loss: 9.8625 - val_categorical_accuracy: 0.0163\n",
      "Epoch 55/120\n",
      "30/30 [==============================] - 88s 3s/step - loss: 9.9088 - categorical_accuracy: 0.0158 - val_loss: 10.0234 - val_categorical_accuracy: 0.0138\n",
      "Epoch 56/120\n",
      "30/30 [==============================] - 89s 3s/step - loss: 10.0999 - categorical_accuracy: 0.0258 - val_loss: 9.8382 - val_categorical_accuracy: 0.0150\n",
      "Epoch 57/120\n",
      "30/30 [==============================] - 88s 3s/step - loss: 9.7803 - categorical_accuracy: 0.0083 - val_loss: 10.2999 - val_categorical_accuracy: 0.0400\n",
      "Epoch 58/120\n",
      "30/30 [==============================] - 88s 3s/step - loss: 10.2605 - categorical_accuracy: 0.0133 - val_loss: 10.1202 - val_categorical_accuracy: 0.0113\n",
      "Epoch 59/120\n",
      "30/30 [==============================] - 88s 3s/step - loss: 10.2147 - categorical_accuracy: 0.0133 - val_loss: 10.4900 - val_categorical_accuracy: 0.0088\n",
      "Epoch 60/120\n",
      "30/30 [==============================] - 88s 3s/step - loss: 10.0038 - categorical_accuracy: 0.0150 - val_loss: 9.9649 - val_categorical_accuracy: 0.0075\n",
      "Epoch 61/120\n",
      "30/30 [==============================] - 88s 3s/step - loss: 10.1163 - categorical_accuracy: 0.0167 - val_loss: 10.0346 - val_categorical_accuracy: 0.0100\n",
      "Epoch 62/120\n",
      "30/30 [==============================] - 88s 3s/step - loss: 10.0853 - categorical_accuracy: 0.0242 - val_loss: 9.6866 - val_categorical_accuracy: 0.0188\n",
      "Epoch 63/120\n",
      "30/30 [==============================] - 88s 3s/step - loss: 9.9885 - categorical_accuracy: 0.0117 - val_loss: 10.1744 - val_categorical_accuracy: 0.0063\n",
      "Epoch 64/120\n",
      "30/30 [==============================] - 88s 3s/step - loss: 9.9638 - categorical_accuracy: 0.0183 - val_loss: 9.9009 - val_categorical_accuracy: 0.0150\n",
      "Epoch 65/120\n",
      "30/30 [==============================] - 88s 3s/step - loss: 10.0957 - categorical_accuracy: 0.0142 - val_loss: 10.0138 - val_categorical_accuracy: 0.0225\n",
      "Epoch 66/120\n",
      "30/30 [==============================] - 88s 3s/step - loss: 9.8473 - categorical_accuracy: 0.0208 - val_loss: 10.1659 - val_categorical_accuracy: 0.0175\n",
      "Epoch 67/120\n",
      "30/30 [==============================] - 88s 3s/step - loss: 10.1042 - categorical_accuracy: 0.0125 - val_loss: 9.8788 - val_categorical_accuracy: 0.0113\n",
      "Epoch 68/120\n",
      "30/30 [==============================] - 88s 3s/step - loss: 10.0415 - categorical_accuracy: 0.0150 - val_loss: 10.0789 - val_categorical_accuracy: 0.0213\n",
      "Epoch 69/120\n",
      "30/30 [==============================] - 88s 3s/step - loss: 10.0486 - categorical_accuracy: 0.0142 - val_loss: 9.8862 - val_categorical_accuracy: 0.0188\n",
      "Epoch 70/120\n",
      "30/30 [==============================] - 88s 3s/step - loss: 9.7842 - categorical_accuracy: 0.0150 - val_loss: 9.7167 - val_categorical_accuracy: 0.0225\n",
      "Epoch 71/120\n",
      "30/30 [==============================] - 88s 3s/step - loss: 9.8554 - categorical_accuracy: 0.0167 - val_loss: 9.9549 - val_categorical_accuracy: 0.0138\n",
      "Epoch 72/120\n",
      "30/30 [==============================] - 88s 3s/step - loss: 9.7700 - categorical_accuracy: 0.0242 - val_loss: 10.1246 - val_categorical_accuracy: 0.0175\n",
      "Epoch 73/120\n",
      "30/30 [==============================] - 88s 3s/step - loss: 10.0050 - categorical_accuracy: 0.0092 - val_loss: 9.7548 - val_categorical_accuracy: 0.0163\n",
      "Epoch 74/120\n",
      "30/30 [==============================] - 88s 3s/step - loss: 10.0765 - categorical_accuracy: 0.0133 - val_loss: 10.2808 - val_categorical_accuracy: 0.0100\n",
      "Epoch 75/120\n",
      "30/30 [==============================] - 88s 3s/step - loss: 9.9629 - categorical_accuracy: 0.0158 - val_loss: 10.2098 - val_categorical_accuracy: 0.0225\n",
      "Epoch 76/120\n",
      "30/30 [==============================] - 88s 3s/step - loss: 9.9912 - categorical_accuracy: 0.0208 - val_loss: 9.8500 - val_categorical_accuracy: 0.0088\n",
      "Epoch 77/120\n",
      "30/30 [==============================] - 87s 3s/step - loss: 9.9479 - categorical_accuracy: 0.0133 - val_loss: 10.2338 - val_categorical_accuracy: 0.0100\n",
      "Epoch 78/120\n",
      "30/30 [==============================] - 89s 3s/step - loss: 9.9367 - categorical_accuracy: 0.0167 - val_loss: 10.1812 - val_categorical_accuracy: 0.0175\n",
      "Epoch 79/120\n",
      "30/30 [==============================] - 88s 3s/step - loss: 10.1371 - categorical_accuracy: 0.0108 - val_loss: 9.8153 - val_categorical_accuracy: 0.0150\n",
      "Epoch 80/120\n",
      "30/30 [==============================] - 88s 3s/step - loss: 10.2281 - categorical_accuracy: 0.0108 - val_loss: 9.9165 - val_categorical_accuracy: 0.0138\n",
      "Epoch 81/120\n",
      "30/30 [==============================] - 88s 3s/step - loss: 9.8187 - categorical_accuracy: 0.0150 - val_loss: 9.7483 - val_categorical_accuracy: 0.0163\n",
      "Epoch 82/120\n",
      "30/30 [==============================] - 2601s 87s/step - loss: 10.0579 - categorical_accuracy: 0.0125 - val_loss: 10.2412 - val_categorical_accuracy: 0.0388\n",
      "Epoch 83/120\n",
      "30/30 [==============================] - 102s 3s/step - loss: 10.1080 - categorical_accuracy: 0.0092 - val_loss: 9.9446 - val_categorical_accuracy: 0.0125\n",
      "Epoch 84/120\n",
      "30/30 [==============================] - 105s 4s/step - loss: 9.8452 - categorical_accuracy: 0.0125 - val_loss: 10.4858 - val_categorical_accuracy: 0.0100\n",
      "Epoch 85/120\n",
      "30/30 [==============================] - 99s 3s/step - loss: 9.9575 - categorical_accuracy: 0.0117 - val_loss: 10.0208 - val_categorical_accuracy: 0.0038\n",
      "Epoch 86/120\n",
      "30/30 [==============================] - 94s 3s/step - loss: 10.1242 - categorical_accuracy: 0.0158 - val_loss: 9.8735 - val_categorical_accuracy: 0.0138\n",
      "Epoch 87/120\n",
      "30/30 [==============================] - 88s 3s/step - loss: 9.7828 - categorical_accuracy: 0.0125 - val_loss: 9.6671 - val_categorical_accuracy: 0.0163\n",
      "Epoch 88/120\n",
      "30/30 [==============================] - 94s 3s/step - loss: 9.7764 - categorical_accuracy: 0.0242 - val_loss: 10.0288 - val_categorical_accuracy: 0.0075\n",
      "Epoch 89/120\n",
      "30/30 [==============================] - 96s 3s/step - loss: 10.0472 - categorical_accuracy: 0.0133 - val_loss: 9.8711 - val_categorical_accuracy: 0.0138\n",
      "Epoch 90/120\n",
      "30/30 [==============================] - 110s 4s/step - loss: 9.6328 - categorical_accuracy: 0.0133 - val_loss: 9.8788 - val_categorical_accuracy: 0.0225\n",
      "Epoch 91/120\n",
      "30/30 [==============================] - 89s 3s/step - loss: 9.8798 - categorical_accuracy: 0.0117 - val_loss: 10.1393 - val_categorical_accuracy: 0.0150\n",
      "Epoch 92/120\n",
      "30/30 [==============================] - 89s 3s/step - loss: 10.0036 - categorical_accuracy: 0.0133 - val_loss: 9.8196 - val_categorical_accuracy: 0.0150\n",
      "Epoch 93/120\n",
      "30/30 [==============================] - 89s 3s/step - loss: 10.1716 - categorical_accuracy: 0.0142 - val_loss: 10.1610 - val_categorical_accuracy: 0.0200\n",
      "Epoch 94/120\n",
      "30/30 [==============================] - 88s 3s/step - loss: 10.0814 - categorical_accuracy: 0.0158 - val_loss: 9.7463 - val_categorical_accuracy: 0.0163\n",
      "Epoch 95/120\n",
      "30/30 [==============================] - 89s 3s/step - loss: 10.2395 - categorical_accuracy: 0.0175 - val_loss: 9.6317 - val_categorical_accuracy: 0.0250\n",
      "Epoch 96/120\n",
      "30/30 [==============================] - 91s 3s/step - loss: 9.8291 - categorical_accuracy: 0.0233 - val_loss: 9.7944 - val_categorical_accuracy: 0.0175\n",
      "Epoch 97/120\n",
      "30/30 [==============================] - 90s 3s/step - loss: 9.5793 - categorical_accuracy: 0.0150 - val_loss: 10.0069 - val_categorical_accuracy: 0.0138\n",
      "Epoch 98/120\n",
      "30/30 [==============================] - 114s 4s/step - loss: 9.7213 - categorical_accuracy: 0.0133 - val_loss: 9.8403 - val_categorical_accuracy: 0.0175\n",
      "Epoch 99/120\n",
      "30/30 [==============================] - 94s 3s/step - loss: 9.7317 - categorical_accuracy: 0.0175 - val_loss: 10.1128 - val_categorical_accuracy: 0.0125\n",
      "Epoch 100/120\n",
      "30/30 [==============================] - 95s 3s/step - loss: 9.9772 - categorical_accuracy: 0.0200 - val_loss: 10.1673 - val_categorical_accuracy: 0.0200\n",
      "Epoch 101/120\n",
      "30/30 [==============================] - 88s 3s/step - loss: 10.1745 - categorical_accuracy: 0.0067 - val_loss: 9.7098 - val_categorical_accuracy: 0.0113\n",
      "Epoch 102/120\n",
      "30/30 [==============================] - 88s 3s/step - loss: 9.7789 - categorical_accuracy: 0.0192 - val_loss: 10.2258 - val_categorical_accuracy: 0.0075\n",
      "Epoch 103/120\n",
      "30/30 [==============================] - 88s 3s/step - loss: 10.0295 - categorical_accuracy: 0.0142 - val_loss: 10.3057 - val_categorical_accuracy: 0.0175\n",
      "Epoch 104/120\n",
      "30/30 [==============================] - 91s 3s/step - loss: 10.0699 - categorical_accuracy: 0.0133 - val_loss: 9.8218 - val_categorical_accuracy: 0.0100\n",
      "Epoch 105/120\n",
      "30/30 [==============================] - 92s 3s/step - loss: 10.0844 - categorical_accuracy: 0.0125 - val_loss: 9.8534 - val_categorical_accuracy: 0.0175\n",
      "Epoch 106/120\n",
      "30/30 [==============================] - 91s 3s/step - loss: 9.8343 - categorical_accuracy: 0.0158 - val_loss: 9.7643 - val_categorical_accuracy: 0.0175\n",
      "Epoch 107/120\n",
      "30/30 [==============================] - 89s 3s/step - loss: 10.3757 - categorical_accuracy: 0.0142 - val_loss: 10.1144 - val_categorical_accuracy: 0.0300\n",
      "Epoch 108/120\n",
      "30/30 [==============================] - 89s 3s/step - loss: 9.8843 - categorical_accuracy: 0.0108 - val_loss: 9.8891 - val_categorical_accuracy: 0.0188\n",
      "Epoch 109/120\n",
      "30/30 [==============================] - 89s 3s/step - loss: 9.8428 - categorical_accuracy: 0.0175 - val_loss: 10.3923 - val_categorical_accuracy: 0.0150\n",
      "Epoch 110/120\n",
      "30/30 [==============================] - 89s 3s/step - loss: 10.2441 - categorical_accuracy: 0.0092 - val_loss: 10.0754 - val_categorical_accuracy: 0.0025\n",
      "Epoch 111/120\n",
      "30/30 [==============================] - 89s 3s/step - loss: 9.9594 - categorical_accuracy: 0.0167 - val_loss: 9.9033 - val_categorical_accuracy: 0.0138\n",
      "Epoch 112/120\n",
      "30/30 [==============================] - 89s 3s/step - loss: 10.1012 - categorical_accuracy: 0.0133 - val_loss: 9.7124 - val_categorical_accuracy: 0.0113\n",
      "Epoch 113/120\n",
      "30/30 [==============================] - 89s 3s/step - loss: 10.0219 - categorical_accuracy: 0.0133 - val_loss: 9.8536 - val_categorical_accuracy: 0.0125\n",
      "Epoch 114/120\n",
      "30/30 [==============================] - 104s 3s/step - loss: 9.8707 - categorical_accuracy: 0.0183 - val_loss: 9.7724 - val_categorical_accuracy: 0.0125\n",
      "Epoch 115/120\n",
      "30/30 [==============================] - 120s 4s/step - loss: 9.9774 - categorical_accuracy: 0.0125 - val_loss: 9.8700 - val_categorical_accuracy: 0.0225\n",
      "Epoch 116/120\n",
      "30/30 [==============================] - 93s 3s/step - loss: 9.8043 - categorical_accuracy: 0.0117 - val_loss: 10.1181 - val_categorical_accuracy: 0.0125\n",
      "Epoch 117/120\n",
      "30/30 [==============================] - 96s 3s/step - loss: 10.2004 - categorical_accuracy: 0.0158 - val_loss: 9.9206 - val_categorical_accuracy: 0.0175\n",
      "Epoch 118/120\n",
      "30/30 [==============================] - 94s 3s/step - loss: 10.1650 - categorical_accuracy: 0.0092 - val_loss: 10.0377 - val_categorical_accuracy: 0.0213\n",
      "Epoch 119/120\n",
      "30/30 [==============================] - 90s 3s/step - loss: 9.8417 - categorical_accuracy: 0.0158 - val_loss: 9.7604 - val_categorical_accuracy: 0.0113\n",
      "Epoch 120/120\n",
      "30/30 [==============================] - 92s 3s/step - loss: 10.1131 - categorical_accuracy: 0.0092 - val_loss: 9.5828 - val_categorical_accuracy: 0.0288\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a29f63ba8>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_seq.fit_generator(train_data_generator.generate(one_sequence=True), steps_per_epoch = 30, \n",
    "                    epochs=num_epochs,\n",
    "                    validation_data=valid_data_generator.generate(one_sequence=True),\n",
    "                    validation_steps=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_seq.save('model_seq_120ep_300dtrainable.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/flatironschool/anaconda3/envs/learn-env/lib/python3.6/site-packages/keras/utils/data_utils.py:651: DeprecationWarning: `wait_time` is not used anymore.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "10/10 [==============================] - 31s 3s/step - loss: 9.6868 - categorical_accuracy: 0.0100\n",
      "Epoch 2/10\n",
      "10/10 [==============================] - 30s 3s/step - loss: 9.7769 - categorical_accuracy: 0.0125\n",
      "Epoch 3/10\n",
      "10/10 [==============================] - 28s 3s/step - loss: 9.9558 - categorical_accuracy: 0.0150\n",
      "Epoch 4/10\n",
      "10/10 [==============================] - 28s 3s/step - loss: 10.0826 - categorical_accuracy: 0.0175\n",
      "Epoch 5/10\n",
      "10/10 [==============================] - 28s 3s/step - loss: 9.6149 - categorical_accuracy: 0.0150\n",
      "Epoch 6/10\n",
      "10/10 [==============================] - 28s 3s/step - loss: 9.8625 - categorical_accuracy: 0.0175\n",
      "Epoch 7/10\n",
      "10/10 [==============================] - 29s 3s/step - loss: 10.2852 - categorical_accuracy: 0.0125\n",
      "Epoch 8/10\n",
      "10/10 [==============================] - 31s 3s/step - loss: 10.1278 - categorical_accuracy: 0.0075\n",
      "Epoch 9/10\n",
      "10/10 [==============================] - 29s 3s/step - loss: 10.2136 - categorical_accuracy: 0.0300\n",
      "Epoch 10/10\n",
      "10/10 [==============================] - 28s 3s/step - loss: 9.7923 - categorical_accuracy: 0.0125\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'History' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-f0cd15398417>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel_seq_hist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_seq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_data_generator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mone_sequence\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_seq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'categorical_accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m# print(model_seq.history['accuracy'])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'History' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "model_seq_hist = model_seq.fit_generator(valid_data_generator.generate(one_sequence=True), epochs=10, steps_per_epoch=10)\n",
    "\n",
    "# print(model_seq.history['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.010000000149011612, 0.012500000186264515, 0.015000000223517418, 0.017500000447034834, 0.015000000223517418, 0.01750000026077032, 0.012500000186264515, 0.007500000111758709, 0.030000000819563867, 0.012500000186264515]\n"
     ]
    }
   ],
   "source": [
    "print(model_seq.history.history['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
